{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loris997/Assignment/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "fastbook.setup_book()"
      ],
      "metadata": {
        "id": "HhmY7I5M8VJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Artificial Neural Networks\n",
        "\n",
        "Please read the introdcution of neuronal networks of the book *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*, p. 299-316.\n",
        "\n",
        "**Why have neural networks, even though they were invented early on, only now caught on?**\n",
        "\n",
        "The first ANN was introduced in 1943. But back then deifferent reason hindered the success of it: \n",
        "- The computational power required to train neural networks was enormous\n",
        "- There were not a lot of large enough datasets avalable\n",
        "\n",
        "In the early 1980s, new architectures were invented\n",
        "and better training techniques were developed, sparking a revival of interest in\n",
        "connectionism, the study of neural networks. But progress was slow, and by the 1990s\n",
        "other powerful machine learning techniques had been invented, such as support\n",
        "vector machines. These other techiques were more promising and therefore ANNs were one again put on hold. \n",
        "\n",
        "The latest push comes because of the following reasons: \n",
        "- Huge quantity of data available to train neural networks\n",
        "- ANNs frequently outperform other ML techniques on very large and complex\n",
        "problems\n",
        "- Tremendous increase in computing power\n",
        "- Training algorithms have been improved\n",
        "- Some theoretical limitations of ANNs have turned out to be benign in practice\n",
        "- Some ANNs had success: now there is more funding and progress\n",
        "\n",
        "\n",
        "**What is a perceptron and a threshold logic unit (TLU)?**\n",
        "\n",
        "Threshold logic unit: The TLU computes a linear function of f its inputs: z =\n",
        "w1 x1 + w2 x2 + ⋯ + wn xn + b = w⊺ x + b. After that it applies a step function to the result: hw(x) = step(z). The input weights are w and the bias term b.\n",
        "\n",
        "Perceptron: A perceptron is composed of either one or more TLUs organizted in a single layer. Every TLU is connected to every output. \n",
        "\n",
        "**Try to define a linear function and a step function of your choice, use some values of your choice and explain what might be the result of the percepton. (maybe using max. two TLU's)**\n",
        "\n",
        "We define a liner functions with two inputs x1 and x2: y = w1x1 + w2x2 + b, \n",
        "g = w3x1 * w4x2 + b2. \n",
        "We define both w1 and w2 as 0.5, w3 as 0.3 and w4 as 0.9. The bias b1 is set to 0.1 and b2 to 0.5. \n",
        "We use the Heaviside step function: Heaviside(z) =  0 if z < 0, 1 if z > 0. \n",
        "\n",
        "If we now feed the function with x1 = 2 and x2 = 1 the functions looks like this:\n",
        "\n",
        "Y = 0.5 * 2 + 0.5 * 1 + 0.1\n",
        "G = 0.3 * 2 + 0.9 * 1 + 0.5\n",
        "\n",
        "Which results in the following output: y = 1.6 and g = 2.\n",
        "\n",
        "1.6 > 0 and 2 > 0 therefore the Heaviside step function returns: 1 for both functions.\n",
        "\n",
        "**What is a fully connected layer and a output layer?** \n",
        "\n",
        "Fully connected layer: When every TLU is connected to every output. \n",
        "\n",
        "Output layer: The last layer produces the output and is therefore output layer called.\n",
        "\n",
        "**Why can we easily combine the equations of multiple instances into a fully connected layer?**\n",
        "\n",
        "The mathematical operations in a fully connected layer are linear transformations, they can be expressed as matrix multiplication. Therefore it is possible to combine the equations of multiple instances into a fully connected layer. \n",
        "**What problem did Marvin Minsky and Seymour Paper highlight that perceptrons could not solve? What is a possible solution?**\n",
        "\n",
        "They highlightet the exclusive OR (XOR) problem, which wasn't possible to be solved by perceptrons. \n",
        "\n",
        "A possible solution to the problem is multilayer perceptron. \n",
        "\n",
        "**What is a deep neuronal network?** \n",
        "\n",
        "If an ANN contains a deep stack of layers (hidden layers), it is called a deep neuronal network. In 1990 an ANN with two hidden layers was considered deep, nowadays there are ANNs with dozen or hundreds of layers. \n",
        "\n",
        "**What are hidden layers?** \n",
        "\n",
        "A perceptron contains always an input layer and an output layer. If there is another layer between these tow, this is called a hidden layer. \n",
        "\n",
        "**What means feedforward neural network (FNN).**\n",
        "\n",
        "If the signal only flows in one direction, from the input to the output we talk of a feedforward neural network. \n",
        "\n",
        "**Try to explain how backpropagation works!**\n",
        "\n",
        "Backpropagation can be defined in 6 steps:\n",
        "\n",
        "- It handles one mini-batch at a time and goes through this set multiple times. Each pass is called an Epoch.\n",
        "- Each mini-batch enters the network through the input layer. Then the algorithm copmputes the output of all neurons in the first hidden layer. This is repeted till each layer is through and we got the output of the output layer. This process is called the forward pass. \n",
        "- The algorithm measures the output error. \n",
        "- The algorithm computes how much each output bias and each connection to the output layer contributed to the error. \n",
        "- The algorithm then measures how much of these error contributions came from\n",
        "each connection in the layer below. This process is done by working backwards till the input layer is reached. \n",
        "- The algorithm performs a gradient descent step to tweak all the connec‐\n",
        "tion weights in the network.\n",
        "\n",
        "**Why do we need activation functions, wouldn't it be easier just using linear functions?**\n",
        "\n",
        "Activation functions are needed to add ninlinearity into the model. Without that, the neural network would only be a linear function of the input. This would limit the network and wouldn't be able to model complex relationships between input and output. \n",
        "\n",
        "## Ideas for the learning portfolio: \n",
        "\n",
        "1) For example, you could train a single TLU to classify iris flowers based on petal length and width in the !!!pyTorch!! environment.\n",
        "\n",
        "2) You could add to our king county housepricing ML project a neuronal network and compare it to the other models. "
      ],
      "metadata": {
        "id": "_Rdj49uwjuoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "4tF8YDV3T91n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A traditional approach: training a digit classifier and learning pyTorch tensors.\n",
        "\n",
        "For this assignment, I ask you to read the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy) to the beginning of the chapter *Stochastic Gradient Descent (SGD)*. \n",
        "\n",
        "First, try to summarize what we know about pyTorch tensors by trying to predict whether we have a 1 or a 7 in the MNIST dataset using a traditional rule-based programming approach. Therefore use pyTorch tensors for the entire tasks and fulfill the following steps:\n",
        "\n",
        "1) Randomly split the MNIST dataset (1 and 7) into a training dataset and a test dataset in a ratio of 80:20.\n",
        "\n",
        "2) Instead of using an optimal 1 or 7 with the mean over the training dataset, try to calculate the sum of the distances to all instances in the training set for each instance in the test dataset. You can use the L2 norm. \n",
        "\n",
        "3) For each instance in the test set, decide if it is a 1 or 7 and calculate the precision.\n",
        "\n",
        "Do we get a similar good result?\n"
      ],
      "metadata": {
        "id": "h6OwXNEeed93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number 1"
      ],
      "metadata": {
        "id": "PnBczUqhcPTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQi-Y-ZEOC8k",
        "outputId": "477761e1-2fe2-42dc-bd5e-48cbf3e6c88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('training'),Path('testing')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrrgv9OVebAH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path = untar_data(URLs.MNIST)\n",
        "Path.BASE_PATH = path\n",
        "path.ls()\n",
        "\n",
        "(path/'training').ls()\n",
        "\n",
        "ones = (path/'training'/'1').ls().sorted()\n",
        "sevens = (path/'training'/'7').ls().sorted()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing an example of a one"
      ],
      "metadata": {
        "id": "NkUbmJcR8r7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im3_path = ones[500]\n",
        "im3 = Image.open(im3_path)\n",
        "im3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "fuoOLG3Zdf3z",
        "outputId": "3c7f4a68-8457-4147-ab47-5659b762925d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAa0lEQVR4nGNgGNzA4bMWEo8JVdLz53OcGl2+/3DGqVOS7f1e3MaiAlRJRiZG3JL///3HLYnX2Dh8kg/wSZqR7aA1+Ix9yCDsg1OSgeH9HpyS577+/8GAEzxHiTESQkiJA49Szi1e+EyiEgAABV0ZqxHtO7UAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing an example of a seven"
      ],
      "metadata": {
        "id": "cs23K6yg8nTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im3_path = sevens[500]\n",
        "im3 = Image.open(im3_path)\n",
        "im3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "mPKmj2U0hAjI",
        "outputId": "9edd5c8d-7b7d-46f1-e5eb-90b6250e290d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApUlEQVR4nNXOsQpBcRTH8V/qStkMJgZmBlnuxMQs5TkM8gQGbyDPcjfLvckoMcgqg/Gi9L0Z7kKd/y1l8Vs/53fOkf4xgyhJZmXb2neA68REHwBuYc3ARrhOeV+1ut7wcAbYVezDoyfAwvHyHGBTsrEHQN3GwuoTc+/42ErS2HG0BdC3m2mOWaifY1PSKXaMBsDUtSeAS9GxNu9Jy9iB3U7We9/kBf87SqPMs/UeAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the test-train set"
      ],
      "metadata": {
        "id": "E1w3MnWu8t4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_ones, test_set_ones = train_test_split(ones, test_size=0.2,random_state=42)\n",
        "train_set_sevens, test_set_sevens = train_test_split(sevens, test_size=0.2,random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1bK2rIIqV2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number 2"
      ],
      "metadata": {
        "id": "dqLiyVUUX_pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_ones_tensor = [tensor(Image.open(o)) for o in train_set_ones]\n",
        "stacked_train_set_ones_tensor = torch.stack(train_set_ones_tensor).float()/255\n",
        "\n",
        "test_set_ones_tensor = [tensor(Image.open(o)) for o in test_set_ones]\n",
        "stacked_test_set_ones_tensor = torch.stack(test_set_ones_tensor).float()/255\n",
        "\n",
        "\n",
        "train_set_sevens_tensor = [tensor(Image.open(o)) for o in train_set_sevens]\n",
        "stacked_train_set_sevens_tensor = torch.stack(train_set_sevens_tensor).float()/255\n",
        "\n",
        "\n",
        "test_set_sevens_tensor = [tensor(Image.open(o)) for o in test_set_sevens]\n",
        "stacked_test_set_sevens_tensor = torch.stack(test_set_sevens_tensor).float()/255\n"
      ],
      "metadata": {
        "id": "gy4OttuyYDMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))"
      ],
      "metadata": {
        "id": "x4yBg2xBHFcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number 3"
      ],
      "metadata": {
        "id": "Ivi4F3vZEYRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(stacked_train_set_sevens_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83GQClWxFT-",
        "outputId": "a2ccdc8d-7342-4992-8128-193df80ee691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5012"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stacked_train_set_ones_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNd16yjWXEaQ",
        "outputId": "22d773da-3a5d-4f9e-f5b0-c6d6c6caaeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5393"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "amount_ones = 0\n",
        "amount_sevens = 0\n",
        "\n",
        "def is_one(test_instance, train_set):\n",
        "    if is_one(test_instance, stacked_train_set_ones_tensor).sum() < mnist_distance(test_instance, stacked_train_set_sevens_tensor).sum():\n",
        "      return 1\n",
        "    else:\n",
        "      return 7\n",
        "\n",
        "for i in stacked_test_set_ones_tensor:\n",
        "  if is_one(i, stacked_test_set_ones_tensor) == 1:\n",
        "    amount_ones += 1\n",
        "\n",
        "\n",
        "for j in stacked_test_set_sevens_tensor:\n",
        "  if is_one(j, stacked_test_set_sevens_tensor) == 7:\n",
        "    amount_sevens += 1\n",
        "\n",
        "print(amount_ones)\n",
        "print(amount_sevens)\n",
        "\n",
        "accuracy_1s = is_one(amount_ones).float() .mean()\n",
        "accuracy_7s = (1 - is_one(amount_sevens).float()).mean()\n",
        "\n",
        "accuracy_1s,accuracy_7s,(accuracy_1s+accuracy_7s)/2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "tb7ddcae9v_E",
        "outputId": "94e4d06d-ce4b-46de-8c1c-a1efcf18cead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0d8c3e609590>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstacked_test_set_ones_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mis_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_test_set_ones_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mamount_ones\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0d8c3e609590>\u001b[0m in \u001b[0;36mis_one\u001b[0;34m(test_instance, train_set)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_train_set_ones_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmnist_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_train_set_sevens_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-15-0d8c3e609590>\u001b[0m in \u001b[0;36mis_one\u001b[0;34m(test_instance, train_set)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_train_set_ones_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmnist_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_train_set_sevens_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent (SGD)\n",
        "\n",
        "For this exercise I ask you to read the chapter Stochastic Gradient Descent (SGD) from the Google Colab 04_mnist_basics.ipynb in paralell. The chapter starts with a single TLU, compare p. 304 in \"Hands on Machine Learning\". Go through all 7 steps which are an easy example of how Stochastic Gradient Descent works.\n",
        "\n",
        "Our goal is to train a single TLU, which can decide if one number is larger then the other one. Therefore we create 100 random pairs with pyTorch and create a target vector which is eather 1 or 0.\n"
      ],
      "metadata": {
        "id": "ETcE9B9rdcEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((100, 2))\n",
        "y = torch.where(x[:,0] > x[:,1], 1.0, 0.0)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "17qLyDnbpSbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f64a19-64bc-433e-c48a-4ef8aa6a82f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.7878, -0.4780],\n",
            "        [-0.2429, -0.9342],\n",
            "        [-0.2483, -1.2082],\n",
            "        [-0.4777,  0.5201],\n",
            "        [-1.5673, -0.2394],\n",
            "        [ 2.3228, -0.9634],\n",
            "        [ 2.0024,  0.4664],\n",
            "        [ 1.5730, -0.9228],\n",
            "        [ 0.3559, -0.6866],\n",
            "        [-0.4934,  0.2415],\n",
            "        [-1.1109,  0.0915],\n",
            "        [-2.3169, -0.2168],\n",
            "        [-0.3097, -0.3957],\n",
            "        [ 0.8034, -0.6216],\n",
            "        [-0.5920, -0.0631],\n",
            "        [-0.8286,  0.3309],\n",
            "        [ 0.0349,  0.3211],\n",
            "        [ 1.5736, -0.8455],\n",
            "        [ 1.3123,  0.6872],\n",
            "        [-1.0892, -0.3553],\n",
            "        [-1.4181,  0.8963],\n",
            "        [ 0.0499,  2.2667],\n",
            "        [ 1.1790, -0.4345],\n",
            "        [-1.3864, -1.2862],\n",
            "        [-0.8371, -0.9224],\n",
            "        [ 1.8113,  0.1606],\n",
            "        [ 0.3672,  0.1754],\n",
            "        [ 1.3852, -0.4459],\n",
            "        [-1.2024,  0.7078],\n",
            "        [-1.0759,  0.5357],\n",
            "        [ 1.1754,  0.5612],\n",
            "        [-0.4527, -0.7718],\n",
            "        [ 0.1453,  0.2311],\n",
            "        [ 0.0087, -0.1423],\n",
            "        [ 0.1971, -1.1441],\n",
            "        [ 0.3383,  1.6992],\n",
            "        [ 2.8140,  0.3598],\n",
            "        [-0.0898,  0.4584],\n",
            "        [-0.5644,  1.0563],\n",
            "        [-1.4692,  1.4332],\n",
            "        [ 0.7281, -0.7106],\n",
            "        [-0.6021,  0.9604],\n",
            "        [ 0.4048, -1.3543],\n",
            "        [-0.4976,  0.4747],\n",
            "        [-0.1976,  1.2683],\n",
            "        [ 1.2243,  0.0981],\n",
            "        [ 1.7423, -1.3527],\n",
            "        [ 0.2191,  0.5526],\n",
            "        [-0.6788,  0.5743],\n",
            "        [ 0.1877, -0.3576],\n",
            "        [-0.3165,  0.5886],\n",
            "        [-0.8905,  0.4098],\n",
            "        [-0.9864,  0.1233],\n",
            "        [ 0.3499,  0.6173],\n",
            "        [-0.1693,  0.2332],\n",
            "        [ 4.0356,  1.2795],\n",
            "        [ 1.0311, -0.7048],\n",
            "        [ 1.0131, -0.3308],\n",
            "        [ 0.5177,  0.3878],\n",
            "        [-0.5797, -0.1691],\n",
            "        [-0.5733,  0.5069],\n",
            "        [-0.4752, -0.4920],\n",
            "        [ 0.2704, -0.5628],\n",
            "        [ 0.6793,  0.4405],\n",
            "        [-0.3609, -0.0606],\n",
            "        [ 0.0733,  0.8187],\n",
            "        [ 1.4805,  0.3449],\n",
            "        [-1.4241, -0.1163],\n",
            "        [ 0.2176, -0.0467],\n",
            "        [-1.4335, -0.5665],\n",
            "        [-0.4253,  0.2625],\n",
            "        [-1.4391,  0.5214],\n",
            "        [ 1.0414, -0.3997],\n",
            "        [-2.2933,  0.4976],\n",
            "        [-0.4257, -1.3371],\n",
            "        [-0.1933,  0.6526],\n",
            "        [-0.3063, -0.3302],\n",
            "        [-0.9808,  0.1947],\n",
            "        [-1.6535,  0.6814],\n",
            "        [ 1.4611, -0.3098],\n",
            "        [ 0.9633, -0.3095],\n",
            "        [ 0.5712,  1.1179],\n",
            "        [-1.2956,  0.0503],\n",
            "        [-0.5855, -0.3900],\n",
            "        [ 0.9812, -0.6401],\n",
            "        [-0.4908,  0.2080],\n",
            "        [-1.1586, -0.9637],\n",
            "        [-0.3750,  0.8033],\n",
            "        [ 0.7165,  1.5335],\n",
            "        [-1.4510, -0.7861],\n",
            "        [-0.9563, -1.2476],\n",
            "        [-0.7499, -0.5922],\n",
            "        [-0.8146, -1.0212],\n",
            "        [-0.4949, -0.5923],\n",
            "        [ 0.1543,  0.4408],\n",
            "        [-0.1483, -2.3184],\n",
            "        [-0.3980,  1.0805],\n",
            "        [-1.7809,  1.5080],\n",
            "        [ 0.3094, -0.5003],\n",
            "        [ 1.0350,  1.6896]])\n",
            "tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
            "        0., 0., 1., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a function f that is a single TLU, meaning that it summarizes x with weights a, b, c:\n",
        "\n",
        "$ax_0+bx_1+c$\n",
        "\n",
        "In Addition we are using a *sigmoid()* function as step function.\n",
        "\n",
        "$f = \\text{sigmoid}(ax_0+bx_1+c)$"
      ],
      "metadata": {
        "id": "z267w4G48rxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, params):\n",
        "    a,b,c = params\n",
        "    return 1 / (1 + torch.exp(-(a*x[:,0] + b*x[:,1] + c)))\n",
        "\n",
        "print(f(x, [3,-2,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_NvBnCGoLPx",
        "outputId": "f9a0c0bb-f2c2-4a38-c684-37abf90e12fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9.9934e-01, 8.9471e-01, 9.3533e-01, 1.8645e-01, 3.8306e-02, 9.9995e-01, 9.9770e-01, 9.9948e-01, 9.6896e-01, 2.7627e-01, 7.4758e-02, 4.0012e-03, 7.0313e-01, 9.9056e-01, 3.4302e-01, 1.0457e-01,\n",
            "        6.1361e-01, 9.9940e-01, 9.7242e-01, 1.7408e-01, 6.3893e-03, 3.2808e-02, 9.9553e-01, 3.5740e-01, 5.8259e-01, 9.9779e-01, 8.5207e-01, 9.9764e-01, 1.7585e-02, 3.5607e-02, 9.6782e-01, 7.6590e-01,\n",
            "        7.2590e-01, 7.8760e-01, 9.7976e-01, 2.0047e-01, 9.9984e-01, 4.5356e-01, 5.7005e-02, 1.8816e-03, 9.9010e-01, 6.1393e-02, 9.9278e-01, 1.9120e-01, 1.0628e-01, 9.8876e-01, 9.9987e-01, 6.3458e-01,\n",
            "        1.0112e-01, 9.0708e-01, 2.4475e-01, 7.6492e-02, 9.9221e-02, 6.9318e-01, 5.0639e-01, 9.9997e-01, 9.9594e-01, 9.9100e-01, 8.5539e-01, 4.0112e-01, 1.5012e-01, 6.3609e-01, 9.4963e-01, 8.9632e-01,\n",
            "        5.0961e-01, 3.9711e-01, 9.9144e-01, 4.5663e-02, 8.5144e-01, 1.0271e-01, 3.0984e-01, 1.2615e-02, 9.9278e-01, 1.0321e-03, 9.1661e-01, 2.9208e-01, 6.7732e-01, 8.8517e-02, 4.8525e-03, 9.9753e-01,\n",
            "        9.8911e-01, 6.1723e-01, 4.8004e-02, 5.0589e-01, 9.9464e-01, 2.9141e-01, 3.6623e-01, 1.5040e-01, 5.2066e-01, 1.4422e-01, 6.5164e-01, 4.8364e-01, 6.4534e-01, 6.6813e-01, 6.4139e-01, 9.9447e-01,\n",
            "        8.6672e-02, 6.3663e-04, 9.4926e-01, 6.7386e-01])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to our TLU function, we need a loss function. Your task is to implement a absolute difference loss function, $∑|x_i-y_i|$, which counts the number of wrong guesses."
      ],
      "metadata": {
        "id": "UBiKkGKx-jVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mae(preds, targets): return (preds-targets).abs().mean()"
      ],
      "metadata": {
        "id": "cwzyy281wI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to train your single TLU with the absolute difference loss function, use the following code. Choose an appropriate step weight `lr` and try to explain what is happing in each line."
      ],
      "metadata": {
        "id": "eGVNErmbvFxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr = 1 : initializing a learning rate with the value 1.\n",
        "\n",
        "params = torch.randn(3).requires_grad_(): initializing a tensor with random values from a normal distribution. \n",
        "\n",
        "def apply_step(params, prn=True): defining the function apply_step, the functions takes the tensor params as input, and a Boolean varaible prn.\n",
        "\n",
        "preds = f(x, params): computes the predicted output of the model with the function f with the input tensor x and tensor pramas.\n",
        "\n",
        "loss = mae(preds, y): Mean absolute error between predicted output and the true output y. \n",
        "\n",
        "loss.backward(): gradient of the loss\n",
        "\n",
        "params.data -= lr * params.grad.data: updating the tensor prams.it subtracts the the product of the lr and the gradient of pramas.\n",
        "\n",
        "params.grad = None: clears gradients of pramas.\n",
        "\n",
        "if prn: print(params);print(loss.item()): prints parameter values and current loss if prn= True.\n",
        "\n",
        "return preds: returns predicted output tensor preds.\n",
        "\n",
        "for i in range(50): apply_step(params): applies apply_step function 50 times. "
      ],
      "metadata": {
        "id": "bNLVwg0CacYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1\n",
        "params = torch.randn(3).requires_grad_()\n",
        "\n",
        "def apply_step(params, prn=True):\n",
        "    preds = f(x, params)\n",
        "    loss = mae(preds, y)\n",
        "    loss.backward()\n",
        "    params.data -= lr * params.grad.data\n",
        "    params.grad = None\n",
        "    if prn: print(params);print(loss.item())\n",
        "    return preds\n",
        "\n",
        "\n",
        "for i in range(50): apply_step(params)"
      ],
      "metadata": {
        "id": "EB5TYTNmyO3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4251e5e2-e37f-488c-883e-9af7812b44a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3962, 0.1375, 0.0117], requires_grad=True)\n",
            "0.4885903298854828\n",
            "tensor([ 0.5515,  0.0276, -0.0099], requires_grad=True)\n",
            "0.44941380620002747\n",
            "tensor([ 0.6927, -0.0802, -0.0315], requires_grad=True)\n",
            "0.41392049193382263\n",
            "tensor([ 0.8193, -0.1844, -0.0521], requires_grad=True)\n",
            "0.38309672474861145\n",
            "tensor([ 0.9325, -0.2842, -0.0714], requires_grad=True)\n",
            "0.3568616807460785\n",
            "tensor([ 1.0339, -0.3794, -0.0892], requires_grad=True)\n",
            "0.33462992310523987\n",
            "tensor([ 1.1252, -0.4701, -0.1054], requires_grad=True)\n",
            "0.3157126009464264\n",
            "tensor([ 1.2080, -0.5565, -0.1200], requires_grad=True)\n",
            "0.29948994517326355\n",
            "tensor([ 1.2835, -0.6388, -0.1332], requires_grad=True)\n",
            "0.2854542136192322\n",
            "tensor([ 1.3529, -0.7173, -0.1450], requires_grad=True)\n",
            "0.2732030749320984\n",
            "tensor([ 1.4170, -0.7922, -0.1556], requires_grad=True)\n",
            "0.2624201476573944\n",
            "tensor([ 1.4765, -0.8640, -0.1651], requires_grad=True)\n",
            "0.2528561055660248\n",
            "tensor([ 1.5320, -0.9327, -0.1736], requires_grad=True)\n",
            "0.2443130910396576\n",
            "tensor([ 1.5841, -0.9986, -0.1811], requires_grad=True)\n",
            "0.23663274943828583\n",
            "tensor([ 1.6331, -1.0620, -0.1877], requires_grad=True)\n",
            "0.2296871691942215\n",
            "tensor([ 1.6794, -1.1229, -0.1936], requires_grad=True)\n",
            "0.22337223589420319\n",
            "tensor([ 1.7233, -1.1817, -0.1988], requires_grad=True)\n",
            "0.2176024317741394\n",
            "tensor([ 1.7650, -1.2384, -0.2034], requires_grad=True)\n",
            "0.2123069316148758\n",
            "tensor([ 1.8048, -1.2932, -0.2074], requires_grad=True)\n",
            "0.2074267417192459\n",
            "tensor([ 1.8429, -1.3462, -0.2109], requires_grad=True)\n",
            "0.20291225612163544\n",
            "tensor([ 1.8794, -1.3975, -0.2138], requires_grad=True)\n",
            "0.19872146844863892\n",
            "tensor([ 1.9144, -1.4472, -0.2164], requires_grad=True)\n",
            "0.19481870532035828\n",
            "tensor([ 1.9481, -1.4954, -0.2185], requires_grad=True)\n",
            "0.1911734789609909\n",
            "tensor([ 1.9806, -1.5423, -0.2203], requires_grad=True)\n",
            "0.1877593994140625\n",
            "tensor([ 2.0120, -1.5878, -0.2218], requires_grad=True)\n",
            "0.18455366790294647\n",
            "tensor([ 2.0424, -1.6321, -0.2229], requires_grad=True)\n",
            "0.1815364956855774\n",
            "tensor([ 2.0719, -1.6752, -0.2238], requires_grad=True)\n",
            "0.1786905825138092\n",
            "tensor([ 2.1004, -1.7172, -0.2244], requires_grad=True)\n",
            "0.1760006546974182\n",
            "tensor([ 2.1282, -1.7582, -0.2248], requires_grad=True)\n",
            "0.17345331609249115\n",
            "tensor([ 2.1552, -1.7981, -0.2250], requires_grad=True)\n",
            "0.17103664577007294\n",
            "tensor([ 2.1816, -1.8371, -0.2249], requires_grad=True)\n",
            "0.16874012351036072\n",
            "tensor([ 2.2072, -1.8751, -0.2247], requires_grad=True)\n",
            "0.16655422747135162\n",
            "tensor([ 2.2323, -1.9123, -0.2243], requires_grad=True)\n",
            "0.1644706130027771\n",
            "tensor([ 2.2568, -1.9487, -0.2238], requires_grad=True)\n",
            "0.1624816507101059\n",
            "tensor([ 2.2807, -1.9843, -0.2231], requires_grad=True)\n",
            "0.16058051586151123\n",
            "tensor([ 2.3042, -2.0191, -0.2223], requires_grad=True)\n",
            "0.15876103937625885\n",
            "tensor([ 2.3272, -2.0531, -0.2214], requires_grad=True)\n",
            "0.15701764822006226\n",
            "tensor([ 2.3497, -2.0865, -0.2204], requires_grad=True)\n",
            "0.15534521639347076\n",
            "tensor([ 2.3718, -2.1192, -0.2193], requires_grad=True)\n",
            "0.15373915433883667\n",
            "tensor([ 2.3935, -2.1513, -0.2181], requires_grad=True)\n",
            "0.15219523012638092\n",
            "tensor([ 2.4148, -2.1827, -0.2168], requires_grad=True)\n",
            "0.1507095843553543\n",
            "tensor([ 2.4357, -2.2136, -0.2155], requires_grad=True)\n",
            "0.14927871525287628\n",
            "tensor([ 2.4563, -2.2438, -0.2141], requires_grad=True)\n",
            "0.147899329662323\n",
            "tensor([ 2.4766, -2.2736, -0.2126], requires_grad=True)\n",
            "0.1465684175491333\n",
            "tensor([ 2.4965, -2.3027, -0.2110], requires_grad=True)\n",
            "0.14528325200080872\n",
            "tensor([ 2.5162, -2.3314, -0.2095], requires_grad=True)\n",
            "0.1440412849187851\n",
            "tensor([ 2.5356, -2.3596, -0.2078], requires_grad=True)\n",
            "0.1428401619195938\n",
            "tensor([ 2.5547, -2.3873, -0.2061], requires_grad=True)\n",
            "0.14167770743370056\n",
            "tensor([ 2.5735, -2.4145, -0.2044], requires_grad=True)\n",
            "0.1405518651008606\n",
            "tensor([ 2.5921, -2.4413, -0.2027], requires_grad=True)\n",
            "0.1394607573747635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a line of code that counts the number of wrong predictions, rounding your predictions with *round()*."
      ],
      "metadata": {
        "id": "h5_LNc1o_o2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = f(x, params)\n",
        "wrong_predictions = ((preds.round() - y).abs()).sum()\n",
        "wrong_predictions\n"
      ],
      "metadata": {
        "id": "EEUhyhyDxwMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebb058f-8518-48b6-db53-4d3a46e4269b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5., grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}